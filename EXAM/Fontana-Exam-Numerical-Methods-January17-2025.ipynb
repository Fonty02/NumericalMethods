{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbbec92-0280-4823-9bb0-40cdb27db96e",
   "metadata": {},
   "source": [
    "\n",
    "# Regression model\n",
    "# Relative location of CT slices on axial axis\n",
    "\n",
    " The data are available at:\n",
    "\n",
    " https://archive.ics.uci.edu/dataset/206/relative+location+of+ct+slices+on+axial+axis\n",
    "\n",
    "The dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body.\n",
    "\n",
    "The data was retrieved from a set of 53500 CT images from 74 different patients (43 male, 31 female).\n",
    "\n",
    "To exstract the data use the panda routines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d786fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patientId  value0  value1  value2  value3  value4  value5  value6  value7  \\\n",
      "0          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "1          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "2          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "3          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "4          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "\n",
      "   value8  ...  value375  value376  value377  value378  value379  value380  \\\n",
      "0   -0.25  ...     -0.25  0.980381       0.0       0.0       0.0       0.0   \n",
      "1   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "2   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "3   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "4   -0.25  ...     -0.25  0.976833       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   value381  value382  value383  reference  \n",
      "0       0.0     -0.25     -0.25  21.803851  \n",
      "1       0.0     -0.25     -0.25  21.745726  \n",
      "2       0.0     -0.25     -0.25  21.687600  \n",
      "3       0.0     -0.25     -0.25  21.629474  \n",
      "4       0.0     -0.25     -0.25  21.571348  \n",
      "\n",
      "[5 rows x 386 columns]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    " \n",
    "# read the dataset using the compression zip\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/static/public/206/relative+location+of+ct+slices+on+axial+axis.zip',compression='zip')\n",
    " \n",
    "# display dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e76930",
   "metadata": {},
   "source": [
    "We transform the data to a matrix of shape 53500 x 386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1be17b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53500, 386)\n"
     ]
    }
   ],
   "source": [
    "Aall=df.to_numpy()\n",
    "print(Aall.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c2aa7",
   "metadata": {},
   "source": [
    "We add a column of all 1 and we organize the input data by dividing in test set and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed970f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#Add a column of ones at the beginning of the data matrix\n",
    "Aall = np.column_stack([np.ones(Aall.shape[0]), Aall])\n",
    "X = Aall\n",
    "X=np.delete(X,386,1)\n",
    "y = Aall[:,386]\n",
    "print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2188d64d-d9f0-4541-b476-08407c30f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = .9,\n",
    "    test_size = .1,\n",
    "    random_state = 5,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b40466b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48150, 386)\n",
      "(5350, 386)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "rank=np.linalg.matrix_rank(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ddcc6-4e55-439f-9e8f-d170cb5c3568",
   "metadata": {},
   "source": [
    "Use the prepared data to solve the regression model with all the studied techniques.\n",
    "Can we use the normal equation and the QR factorization? If the answer is positive compare the condition numbers of the QR methods and the normal equations. What are the results?\n",
    "\n",
    "\n",
    "Use the funcation scipy.linalg.lstsq and check if all the lapack drivers works.\n",
    "Compare the results changing the initial value cond. The results are the same? What about the execution time?\n",
    "\n",
    "Analyze the singular values and check if it is possible to use a principal component regression procedure. Compute the solution using the singular value decomposition. \n",
    "Can you observe a relation in the chosen singular value and the value of cond of the routine lstsq?\n",
    "\n",
    "Perform the same analysis by preprocessing the data in order to have data from a normal distribution with mean zero  and compute the singular value decomposition on this matrix.\n",
    "\n",
    "Check the performance of the method by computing the least square residual for the training set and the testset. The minimum and the maximum values of the predicted error for both, the training set and the testset.\n",
    "\n",
    "Compute the multiple R-squared: R2_train = 1 - sum( (y - yest)**2)/sum( (y-mean(y))**2 where y are the value to predict and yest are the estimated values for the training set.\n",
    "Compute the value R2_test for the testset.\n",
    "\n",
    "A value of R2 near one means that the constructed model is good.\n",
    "\n",
    "Change the size of the training set and the testing set to 0.7% and 0.3% and repeat the previous steps.\n",
    "\n",
    "Comment the obtained results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e128bbb",
   "metadata": {},
   "source": [
    "<h1> Normal Equation </h1>\n",
    "Normal equation is a method to solve the linear regression problem. It is based on the following formula: \n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "where $\\theta$ is the vector of parameters, $X$ is the matrix of input data, and $y$ is the vector of output data. It is important to note that the matrix $X$ must be full rank, otherwise the inverse of $X^T X$ does not exist, so the normal equation cannot be used.\n",
    "\n",
    "\n",
    "Since the matrix $X$ is not full rank, we cannot use the normal equation to solve the problem. The rank of the matrix $X^T X$ is 375, which is lower than p=min(n,m)=386. The condition number of the matrix $X^T X$ is very high, which means that the matrix is ill-conditioned. The condition number of a matrix is computer as follows: \n",
    "\n",
    "$$\n",
    "\\|A\\|_2 \\cdot \\|A^{-1}\\|_2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7798edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of train data:  375\n",
      "Shape of train data:  (386, 386)\n",
      "Residuals:  5.583855310705728e+19\n",
      "Condition number of normal equation matrix:  1.8704153273192688e+22\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank of train data: \",np.linalg.matrix_rank(X_train.T@X_train))\n",
    "print(\"Shape of train data: \",(X_train.T @ X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6bc6",
   "metadata": {},
   "source": [
    "<h1>QR factorization</h1>\n",
    "The QR factorization is a method to solve the linear regression problem. It is based on the following formula:\n",
    "\n",
    "$$\n",
    "X = QR\n",
    "$$\n",
    "\n",
    "where $X$ is the matrix of input data, $Q$ is an orthogonal matrix, and $R$ is an upper triangular matrix. The solution of the linear regression problem is given by:\n",
    "\n",
    "$$\n",
    "\\theta = R^{-1} Q^T y\n",
    "$$\n",
    "\n",
    "Since $X$ is not full rank, we cannot use the QR factorization to solve the problem. We need to use the Pivot QR factorization, which is based on the following formula:\n",
    "\n",
    "$$\n",
    "X P = QR\n",
    "$$\n",
    "\n",
    "where $P$ is a permutation matrix. The solution of the linear regression problem is given by:\n",
    "\n",
    "$$\n",
    "\\theta = R^{-1} Q^T P^T y\n",
    "$$\n",
    "\n",
    "\n",
    "If we try to compute a solution we can notice that the residual is much lower than the one obtained with the normal equation while the condition number is much higher than the one obtained with the normal equation. Since X_train is singular, the matrix R can have some element near zero, which can lead to numerical instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b9b73e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of QR factorization matrix:  2.4463550697566203e+32\n",
      "Residuals for train set:  1798.3737270942406\n",
      "Maximum error for train set:  49.47129163028276\n",
      "Minimum error for train set:  1.4352963262354024e-12\n",
      "R2 for train set:  0.8653831545119198\n",
      "Residuals for test set:  613.1967475658972\n",
      "Maximum error for test set:  46.98621458423358\n",
      "Minimum error for test set:  0.0023313738117138882\n",
      "R2 for test set:  0.8603214800189586\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg as la\n",
    "Q,R,P = la.qr(X_train, pivoting=True, mode='economic') #economic -> Q is m x k, R is k x n where k=min(m,n)\n",
    "print(\"Condition number of QR factorization matrix: \",np.linalg.cond(R,2))\n",
    "#truncate R and Q to rank\n",
    "R_trunc = R[:rank, :rank]\n",
    "Q_trunc = Q[:, :rank]\n",
    "QTb = Q_trunc.T @ y_train\n",
    "theta_permuted=np.zeros(X_train.shape[1])\n",
    "theta_permuted [:rank]= la.solve_triangular(R_trunc, QTb) #store the solution in the first rank columns\n",
    "#restore original order\n",
    "theta = np.zeros_like(theta_permuted)\n",
    "theta[P] = theta_permuted #permute the elements of theta_permuted to get the solution\n",
    "\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#THIS model is quite good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5899e6",
   "metadata": {},
   "source": [
    "<h1> SVD: Singular Value Decomposition </h1>\n",
    "\n",
    "The Singular Value Decomposition (SVD) can also be used to solve the linear regression problem. It is based on the following formula:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $X$ is the matrix of input data, $U$ is an orthogonal matrix, $Sigma$ is a rectangular, and $V$ is an orthogonal matrix. The solution of the linear regression problem is given by:\n",
    "\n",
    "$$\n",
    "theta = V_r D^{-1} U_r^T y\n",
    "$$\n",
    "\n",
    "where $V_r$ is the matrix containing the first r columns of $V$, $U_r$ is the matrix containing the first r columns of $U$, and $D$ is the diagonal matrix containing the first r singular values, which are the non-zeros one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7812cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of X_train:  375\n",
      "Number of non zero singular values:  386\n",
      "Residuals for train set:  1798.3737270942406\n",
      "Maximum error for train set:  49.47129163028183\n",
      "Minimum error for train set:  5.613287612504791e-13\n",
      "R2 for train set:  0.8653831545119198\n",
      "Residuals for test set:  613.1967475658979\n",
      "Maximum error for test set:  46.98621458423264\n",
      "Minimum error for test set:  0.0023313738110175564\n",
      "R2 for test set:  0.8603214800189583\n"
     ]
    }
   ],
   "source": [
    "U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "print(\"Rank of X_train: \",rank)\n",
    "print(\"Number of non zero singular values: \",np.count_nonzero(S))\n",
    "#S contains all singular values. If we consider only the non zero singular values, they should be equal to the rank of X_train. In this case this is not true, probably due to numerical errors.\n",
    "# I will use later the PCR and Euckardt-Young theorem to remove noise\n",
    "\n",
    "\n",
    "# So let's consider the first r=rank(X_train) singular values\n",
    "U_r = U[:, :rank]\n",
    "Vt_r = Vt[:rank, :]\n",
    "S_r = np.diag(S)[:rank, :rank]\n",
    "S_r_inv = np.linalg.inv(S_r)\n",
    "theta = Vt_r.T @ S_r_inv @ U_r.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d964e",
   "metadata": {},
   "source": [
    "<h1>Scipy.linalg.lstsq</h1>\n",
    "The function scipy.linalg.lstsq is a method to solve the least squares problem. It uses different LAPACK drivers to solve the problem. The available drivers are:\n",
    "\n",
    "1. **`gelsd`**:\n",
    "   - **Description**: The `gelsd` driver solves the **least squares problem** using **Singular Value Decomposition (SVD)** and Divide-and-Conquer method.\n",
    "\n",
    "2. **`gelsy`**:\n",
    "   - **Description**: The `gelsy` driver solves the **least squares problem** using **QR decomposition with column pivoting**.\n",
    "  \n",
    "\n",
    "3. **`gelss`**:\n",
    "   - **Description**:The `gelss` driver solves the **least squares problem** using **Singular Value Decomposition (SVD)**.\n",
    "\n",
    "**Divide and conquer** is an optimization technique used to compute the **Singular Value Decomposition (SVD)** more efficiently.\n",
    "\n",
    "- **Concept**: Divide and conquer breaks down the SVD problem into smaller, more manageable subproblems. Instead of performing the full SVD computation at once, the algorithm recursively divides the matrix into smaller parts and computes the decomposition in stages, merging results as it progresses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "12d787e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'gelsd', 'condition': 0.1, 'AVGtime': 2.509533643722534, 'residuals_train': 7079.003325567669, 'Min error residuals_train': 0.00016317329873416497, 'Max error residuals_train': 86.12879856814983, 'R2_train': -1.085853217608597, 'residuals_test': 2361.142356336503, 'Min error residuals_test': 0.0015384355100849234, 'Max error residuals_test': 86.21332456608175, 'R2_test': -1.0709722681710114}\n",
      "{'driver': 'gelss', 'condition': 0.1, 'AVGtime': 2.30238823890686, 'residuals_train': 7079.00332556767, 'Min error residuals_train': 0.00016317329874482311, 'Max error residuals_train': 86.12879856814982, 'R2_train': -1.085853217608597, 'residuals_test': 2361.142356336503, 'Min error residuals_test': 0.0015384355101133451, 'Max error residuals_test': 86.21332456608174, 'R2_test': -1.0709722681710114}\n",
      "{'driver': 'gelsy', 'condition': 0.1, 'AVGtime': 3.4240085601806642, 'residuals_train': 7079.030051187763, 'Min error residuals_train': 0.001103681757442132, 'Max error residuals_train': 86.13955960940007, 'R2_train': -1.0858689672341675, 'residuals_test': 2361.119587258657, 'Min error residuals_test': 0.0034863125009110263, 'Max error residuals_test': 86.22408570665593, 'R2_test': -1.070932326571298}\n",
      "{'driver': 'gelsd', 'condition': 0.01, 'AVGtime': 2.3429636478424074, 'residuals_train': 2852.209177373248, 'Min error residuals_train': 0.0001833803512454324, 'Max error residuals_train': 59.94903873041823, 'R2_train': 0.6613880686470945, 'residuals_test': 947.1567219811014, 'Min error residuals_test': 0.002341357849196868, 'Max error residuals_test': 57.95973924838983, 'R2_test': 0.6667473351505734}\n",
      "{'driver': 'gelss', 'condition': 0.01, 'AVGtime': 2.2799887657165527, 'residuals_train': 2852.2091773732373, 'Min error residuals_train': 0.00018338034929143987, 'Max error residuals_train': 59.9490387304176, 'R2_train': 0.6613880686470971, 'residuals_test': 947.1567219810992, 'Min error residuals_test': 0.002341357847889469, 'Max error residuals_test': 57.959739248389454, 'R2_test': 0.6667473351505749}\n",
      "{'driver': 'gelsy', 'condition': 0.01, 'AVGtime': 3.1505623340606688, 'residuals_train': 7079.030051187763, 'Min error residuals_train': 0.001103681757442132, 'Max error residuals_train': 86.13955960940007, 'R2_train': -1.0858689672341675, 'residuals_test': 2361.119587258657, 'Min error residuals_test': 0.0034863125009110263, 'Max error residuals_test': 86.22408570665593, 'R2_test': -1.070932326571298}\n",
      "{'driver': 'gelsd', 'condition': 0.0001, 'AVGtime': 2.0414567947387696, 'residuals_train': 1798.3994734141352, 'Min error residuals_train': 1.8338172878884507e-05, 'Max error residuals_train': 49.47573432749514, 'R2_train': 0.8653793000147989, 'residuals_test': 613.1715704956185, 'Min error residuals_test': 0.0014430106109344365, 'Max error residuals_test': 46.9891455047549, 'R2_test': 0.8603329498244735}\n",
      "{'driver': 'gelss', 'condition': 0.0001, 'AVGtime': 2.551532030105591, 'residuals_train': 1798.3994734141352, 'Min error residuals_train': 1.8338172111498352e-05, 'Max error residuals_train': 49.475734327497754, 'R2_train': 0.8653793000147989, 'residuals_test': 613.1715704956163, 'Min error residuals_test': 0.0014430106120997266, 'Max error residuals_test': 46.98914550475641, 'R2_test': 0.8603329498244745}\n",
      "{'driver': 'gelsy', 'condition': 0.0001, 'AVGtime': 3.5156598567962645, 'residuals_train': 1798.3882302496897, 'Min error residuals_train': 8.989160991035305e-05, 'Max error residuals_train': 49.47471047246335, 'R2_train': 0.8653809832425342, 'residuals_test': 613.1948421789593, 'Min error residuals_test': 0.0006585745238822938, 'Max error residuals_test': 46.98724460469762, 'R2_test': 0.860322348064052}\n",
      "{'driver': 'gelsd', 'condition': 1e-08, 'AVGtime': 2.8813243865966798, 'residuals_train': 1798.3737270942404, 'Min error residuals_train': 1.8260948309034575e-12, 'Max error residuals_train': 49.47129163027922, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475659, 'Min error residuals_test': 0.002331373807535897, 'Max error residuals_test': 46.986214584231135, 'R2_test': 0.8603214800189574}\n",
      "{'driver': 'gelss', 'condition': 1e-08, 'AVGtime': 2.25536527633667, 'residuals_train': 1798.3737270942404, 'Min error residuals_train': 4.050093593832571e-13, 'Max error residuals_train': 49.471291630281804, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475658978, 'Min error residuals_test': 0.002331373811045978, 'Max error residuals_test': 46.98621458423261, 'R2_test': 0.8603214800189584}\n",
      "{'driver': 'gelsy', 'condition': 1e-08, 'AVGtime': 3.423196887969971, 'residuals_train': 1798.3737270942406, 'Min error residuals_train': 1.3287149158713873e-12, 'Max error residuals_train': 49.471291630282785, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475658972, 'Min error residuals_test': 0.002331373811642834, 'Max error residuals_test': 46.986214584233636, 'R2_test': 0.8603214800189586}\n",
      "{'driver': 'gelsd', 'condition': 1e-12, 'AVGtime': 2.2828887462615968, 'residuals_train': 1798.3737270942404, 'Min error residuals_train': 1.8260948309034575e-12, 'Max error residuals_train': 49.47129163027922, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475659, 'Min error residuals_test': 0.002331373807535897, 'Max error residuals_test': 46.986214584231135, 'R2_test': 0.8603214800189574}\n",
      "{'driver': 'gelss', 'condition': 1e-12, 'AVGtime': 2.127072048187256, 'residuals_train': 1798.3737270942404, 'Min error residuals_train': 4.050093593832571e-13, 'Max error residuals_train': 49.471291630281804, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475658978, 'Min error residuals_test': 0.002331373811045978, 'Max error residuals_test': 46.98621458423261, 'R2_test': 0.8603214800189584}\n",
      "{'driver': 'gelsy', 'condition': 1e-12, 'AVGtime': 3.3079989910125733, 'residuals_train': 1798.3737270942406, 'Min error residuals_train': 1.3287149158713873e-12, 'Max error residuals_train': 49.471291630282785, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475658972, 'Min error residuals_test': 0.002331373811642834, 'Max error residuals_test': 46.986214584233636, 'R2_test': 0.8603214800189586}\n",
      "{'driver': 'gelsd', 'condition': 1e-16, 'AVGtime': 2.746853828430176, 'residuals_train': 1798.3737270942404, 'Min error residuals_train': 1.8260948309034575e-12, 'Max error residuals_train': 49.47129163027922, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475659, 'Min error residuals_test': 0.002331373807535897, 'Max error residuals_test': 46.986214584231135, 'R2_test': 0.8603214800189574}\n",
      "{'driver': 'gelss', 'condition': 1e-16, 'AVGtime': 2.2658831596374513, 'residuals_train': 1798.3737270942404, 'Min error residuals_train': 4.050093593832571e-13, 'Max error residuals_train': 49.471291630281804, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475658978, 'Min error residuals_test': 0.002331373811045978, 'Max error residuals_test': 46.98621458423261, 'R2_test': 0.8603214800189584}\n",
      "{'driver': 'gelsy', 'condition': 1e-16, 'AVGtime': 3.748401927947998, 'residuals_train': 1798.3737270942406, 'Min error residuals_train': 1.3287149158713873e-12, 'Max error residuals_train': 49.471291630282785, 'R2_train': 0.8653831545119198, 'residuals_test': 613.1967475658972, 'Min error residuals_test': 0.002331373811642834, 'Max error residuals_test': 46.986214584233636, 'R2_test': 0.8603214800189586}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "conditions=[1e-1,1e-2,1e-4,1e-8,1e-12,1e-16]\n",
    "values={}\n",
    "for condition in conditions:\n",
    "    #gelsd\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsd = la.lstsq(X_train, y_train,lapack_driver='gelsd',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    \n",
    "    values['driver']='gelsd'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsd\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsd\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelss\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelss = la.lstsq(X_train, y_train,lapack_driver='gelss',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelss'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelss\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelss\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelsy\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsy = la.lstsq(X_train, y_train,lapack_driver='gelsy',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelsy'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsy\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsy\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11530dbc",
   "metadata": {},
   "source": [
    "We can notice that, decreasing the value of cond, the residual error decreases. Regarding the execution time, we can notiche that in general the gelsy driver is slower than the other two. The execution time tends to increase with the decrease of the value of cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64610c4",
   "metadata": {},
   "source": [
    "<h1> PCR: Principal Component Regression </h1>\n",
    "\n",
    "PCR can be used to solve the linear regression problem. It is based on the following formula:\n",
    "\n",
    "$$\n",
    "X_k = U_k \\Sigma_k V_k^T\n",
    "$$\n",
    "\n",
    "where $X_k$ is an approximation of the matrix of input data, $U_k$ contains the first k columns of $U$, $\\Sigma_k$ contains the first k singular values, and $V_k$ contains the first k columns of $V$.\n",
    "\n",
    "The problem is to find the best value of k. There are several methods that we can use like:\n",
    "\n",
    "\n",
    "1. **Mixed Error**: We can use the mixed error to find the best value of k. The criterion is based on the following formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\|X - X_k\\|_2}{\\|X\\|_2+1} = \\frac{\\sigma_{k+1}}{\\sigma_1 + 1} \\leq \\text{tol}\n",
    "$$\n",
    "\n",
    "When we find k+1 so that the criterion is satisfied, we can use k as the best value of k.\n",
    "\n",
    "2. **Scree Plot**: We can use the scree plot to find the best value of k. The scree plot shows the singular values in decreasing order. We can find the best value of k by looking at the point where the curve starts to flatten.\n",
    "\n",
    "3. **Cumulative Percentage of Variance**: We can use the cumulative percentage of variance to find the best value of k. The cumulative percentage of variance is given by:\n",
    "\n",
    "$$\n",
    "\\frac{100* \\sum_{i=1}^k \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2} >= \\text{percentage}\n",
    "$$\n",
    "\n",
    "When we find k so that the criterion is satisfied, we can use k as the best value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "887cd09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 374  with mixed error criterion and tol= 1e-06\n",
      "Sigma_k= 0.24235413263899452\n",
      "Residuals for train set:  1798.3737432334817\n",
      "Maximum error for train set:  49.47135129405567\n",
      "Minimum error for train set:  8.885552780668604e-06\n",
      "R2 for train set:  0.8653831520957216\n",
      "Residuals for test set:  613.1967323323433\n",
      "Maximum error for test set:  46.9862610598538\n",
      "Minimum error for test set:  0.0023566596409949625\n",
      "R2 for test set:  0.8603214869589831\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 5  with Screep plot\n",
      "Sigma_k= 250.9209568227013\n",
      "Residuals for train set:  4314.872172761188\n",
      "Maximum error for train set:  81.8056017452396\n",
      "Minimum error for train set:  4.238081472607291e-05\n",
      "R2 for train set:  0.22504697175187627\n",
      "Residuals for test set:  1430.3535335393976\n",
      "Maximum error for test set:  72.85619426228541\n",
      "Minimum error for test set:  0.004824564473430826\n",
      "R2 for test set:  0.23999451660000515\n",
      "k= 1 with cumulative percentage of variance and p= 0.99\n",
      "Sigma_k= 688.74133812504\n",
      "Residuals for train set:  7079.00332556767\n",
      "Maximum error for train set:  86.12879856814982\n",
      "Minimum error for train set:  0.00016317329874482311\n",
      "R2 for train set:  -1.085853217608597\n",
      "Residuals for test set:  2361.142356336503\n",
      "Maximum error for test set:  86.21332456608174\n",
      "Minimum error for test set:  0.0015384355101133451\n",
      "R2 for test set:  -1.0709722681710114\n"
     ]
    }
   ],
   "source": [
    "#1: mixed error criterion\n",
    "tol=1e-6\n",
    "k=np.argmax(S/(S[0]+1)<tol)-1  #argmax returns the first occurrence of the that satisfies the condition and this is k+1\n",
    "print(\"k=\",k,\" with mixed error criterion and tol=\",tol)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "#2: Scree Plot\n",
    "\n",
    "k1 = KneeLocator(range(len(S)), S, curve='convex', direction='decreasing')\n",
    "plt.plot(range(len(S)), S)\n",
    "plt.scatter(k1.elbow, S[k1.elbow], c='red', s=100, alpha=0.5)\n",
    "plt.xlabel('Singular value index')\n",
    "plt.ylabel('Singular value')\n",
    "plt.title('Scree plot')\n",
    "plt.legend(['Singular values', 'Elbow'])\n",
    "plt.show()\n",
    "k=k1.elbow\n",
    "print(\"k=\",k,\" with Screep plot\")\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#3: Cumulative percentage of variance\n",
    "p=0.99\n",
    "total_sum=np.sum(S**2)\n",
    "cumulative_sum=np.cumsum(S**2)\n",
    "k=np.argmax((cumulative_sum/total_sum)>p)\n",
    "print(\"k=\",k,\"with cumulative percentage of variance and p=\",p)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eceb2d",
   "metadata": {},
   "source": [
    "When the value of the last singular values increas, the value of the residual also increases. This happen also for the cond number in ltsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5d153",
   "metadata": {},
   "source": [
    "<h1> NOW WE CHANGE THE PERCENTAGE OF THE TRAINING SET AND THE TESTING SET </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "48d9d678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of train data:  374\n",
      "Shape of train data:  (386, 386)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = .7,\n",
    "    test_size = .3,\n",
    "    random_state = 5,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "rank=np.linalg.matrix_rank(X_train)\n",
    "print(\"Rank of train data: \",rank)\n",
    "print(\"Shape of train data: \",(X_train.T @ X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbcd65",
   "metadata": {},
   "source": [
    "Since we have no full rank, we cannot solve with normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rank of train data: \",np.linalg.matrix_rank(X_train.T@X_train))\n",
    "print(\"Shape of train data: \",(X_train.T @ X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e72da",
   "metadata": {},
   "source": [
    "QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "Q,R,P = la.qr(X_train, pivoting=True, mode='economic') #economic -> Q is m x k, R is k x n where k=min(m,n)\n",
    "print(\"Condition number of QR factorization matrix: \",np.linalg.cond(R,2))\n",
    "#truncate R and Q to rank\n",
    "R_trunc = R[:rank, :rank]\n",
    "Q_trunc = Q[:, :rank]\n",
    "QTb = Q_trunc.T @ y_train\n",
    "theta_permuted=np.zeros(X_train.shape[1])\n",
    "theta_permuted [:rank]= la.solve_triangular(R_trunc, QTb) #store the solution in the first rank columns\n",
    "#restore original order\n",
    "theta = np.zeros_like(theta_permuted)\n",
    "theta[P] = theta_permuted #permute the elements of theta_permuted to get the solution\n",
    "\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#THIS model is quite good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ec521",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "print(\"Rank of X_train: \",rank)\n",
    "print(\"Number of non zero singular values: \",np.count_nonzero(S))\n",
    "#S contains all singular values. If we consider only the non zero singular values, they should be equal to the rank of X_train. In this case this is not true, probably due to numerical errors.\n",
    "# I will use later the PCR and Euckardt-Young theorem to remove noise\n",
    "\n",
    "\n",
    "# So let's consider the first r=rank(X_train) singular values\n",
    "U_r = U[:, :rank]\n",
    "Vt_r = Vt[:rank, :]\n",
    "S_r = np.diag(S)[:rank, :rank]\n",
    "S_r_inv = np.linalg.inv(S_r)\n",
    "theta = Vt_r.T @ S_r_inv @ U_r.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19af174",
   "metadata": {},
   "source": [
    "LAPACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373aeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "conditions=[1e-1,1e-2,1e-4,1e-8,1e-12,1e-16]\n",
    "values={}\n",
    "for condition in conditions:\n",
    "    #gelsd\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsd = la.lstsq(X_train, y_train,lapack_driver='gelsd',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    \n",
    "    values['driver']='gelsd'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsd\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsd\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelss\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelss = la.lstsq(X_train, y_train,lapack_driver='gelss',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelss'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelss\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelss\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelsy\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsy = la.lstsq(X_train, y_train,lapack_driver='gelsy',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelsy'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsy\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsy\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418cf2d",
   "metadata": {},
   "source": [
    "PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: mixed error criterion\n",
    "tol=1e-6\n",
    "k=np.argmax(S/(S[0]+1)<tol)-1  #argmax returns the first occurrence of the that satisfies the condition and this is k+1\n",
    "print(\"k=\",k,\" with mixed error criterion and tol=\",tol)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "#2: Scree Plot\n",
    "\n",
    "k1 = KneeLocator(range(len(S)), S, curve='convex', direction='decreasing')\n",
    "plt.plot(range(len(S)), S)\n",
    "plt.scatter(k1.elbow, S[k1.elbow], c='red', s=100, alpha=0.5)\n",
    "plt.xlabel('Singular value index')\n",
    "plt.ylabel('Singular value')\n",
    "plt.title('Scree plot')\n",
    "plt.legend(['Singular values', 'Elbow'])\n",
    "plt.show()\n",
    "k=k1.elbow\n",
    "print(\"k=\",k,\" with Screep plot\")\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#3: Cumulative percentage of variance\n",
    "p=0.99\n",
    "total_sum=np.sum(S**2)\n",
    "cumulative_sum=np.cumsum(S**2)\n",
    "k=np.argmax((cumulative_sum/total_sum)>p)\n",
    "print(\"k=\",k,\"with cumulative percentage of variance and p=\",p)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d5263",
   "metadata": {},
   "source": [
    "<h1> Mean 0</h1>\n",
    "<h4> Train set 0.9 and test set 0.1 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7a72e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of train data:  374\n",
      "Shape of train data:  (48150, 386)\n"
     ]
    }
   ],
   "source": [
    "X = X - np.mean(X, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = .9,\n",
    "    test_size = .1,\n",
    "    random_state = 5,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "rank=np.linalg.matrix_rank(X_train)\n",
    "print(\"Rank of train data: \",rank)\n",
    "print(\"Shape of train data: \",(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9fea5",
   "metadata": {},
   "source": [
    "Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ca155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rank of train data: \",np.linalg.matrix_rank(X_train.T@X_train))\n",
    "print(\"Shape of train data: \",(X_train.T @ X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fb3ce",
   "metadata": {},
   "source": [
    "QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e051b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "Q,R,P = la.qr(X_train, pivoting=True, mode='economic') #economic -> Q is m x k, R is k x n where k=min(m,n)\n",
    "print(\"Condition number of QR factorization matrix: \",np.linalg.cond(R,2))\n",
    "#truncate R and Q to rank\n",
    "R_trunc = R[:rank, :rank]\n",
    "Q_trunc = Q[:, :rank]\n",
    "QTb = Q_trunc.T @ y_train\n",
    "theta_permuted=np.zeros(X_train.shape[1])\n",
    "theta_permuted [:rank]= la.solve_triangular(R_trunc, QTb) #store the solution in the first rank columns\n",
    "#restore original order\n",
    "theta = np.zeros_like(theta_permuted)\n",
    "theta[P] = theta_permuted #permute the elements of theta_permuted to get the solution\n",
    "\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#THIS model is quite good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339628b3",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70862aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "print(\"Rank of X_train: \",rank)\n",
    "print(\"Number of non zero singular values: \",np.count_nonzero(S))\n",
    "#S contains all singular values. If we consider only the non zero singular values, they should be equal to the rank of X_train. In this case this is not true, probably due to numerical errors.\n",
    "# I will use later the PCR and Euckardt-Young theorem to remove noise\n",
    "\n",
    "\n",
    "# So let's consider the first r=rank(X_train) singular values\n",
    "U_r = U[:, :rank]\n",
    "Vt_r = Vt[:rank, :]\n",
    "S_r = np.diag(S)[:rank, :rank]\n",
    "S_r_inv = np.linalg.inv(S_r)\n",
    "theta = Vt_r.T @ S_r_inv @ U_r.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6e1dc",
   "metadata": {},
   "source": [
    "LTSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24010dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "conditions=[1e-1,1e-2,1e-4,1e-8,1e-12,1e-16]\n",
    "values={}\n",
    "for condition in conditions:\n",
    "    #gelsd\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsd = la.lstsq(X_train, y_train,lapack_driver='gelsd',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    \n",
    "    values['driver']='gelsd'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsd\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsd\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelss\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelss = la.lstsq(X_train, y_train,lapack_driver='gelss',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelss'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelss\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelss\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelsy\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsy = la.lstsq(X_train, y_train,lapack_driver='gelsy',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelsy'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsy\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsy\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d086a",
   "metadata": {},
   "source": [
    "PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bebb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: mixed error criterion\n",
    "tol=1e-6\n",
    "k=np.argmax(S/(S[0]+1)<tol)-1  #argmax returns the first occurrence of the that satisfies the condition and this is k+1\n",
    "print(\"k=\",k,\" with mixed error criterion and tol=\",tol)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "#2: Scree Plot\n",
    "\n",
    "k1 = KneeLocator(range(len(S)), S, curve='convex', direction='decreasing')\n",
    "plt.plot(range(len(S)), S)\n",
    "plt.scatter(k1.elbow, S[k1.elbow], c='red', s=100, alpha=0.5)\n",
    "plt.xlabel('Singular value index')\n",
    "plt.ylabel('Singular value')\n",
    "plt.title('Scree plot')\n",
    "plt.legend(['Singular values', 'Elbow'])\n",
    "plt.show()\n",
    "k=k1.elbow\n",
    "print(\"k=\",k,\" with Screep plot\")\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#3: Cumulative percentage of variance\n",
    "p=0.99\n",
    "total_sum=np.sum(S**2)\n",
    "cumulative_sum=np.cumsum(S**2)\n",
    "k=np.argmax((cumulative_sum/total_sum)>p)\n",
    "print(\"k=\",k,\"with cumulative percentage of variance and p=\",p)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06664a0c",
   "metadata": {},
   "source": [
    "CHANGE DATASET DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = .9,\n",
    "    test_size = .3,\n",
    "    random_state = 5,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "rank=np.linalg.matrix_rank(X_train)\n",
    "print(\"Rank of train data: \",rank)\n",
    "print(\"Shape of train data: \",(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880be18",
   "metadata": {},
   "source": [
    "Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rank of train data: \",np.linalg.matrix_rank(X_train.T@X_train))\n",
    "print(\"Shape of train data: \",(X_train.T @ X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf5942",
   "metadata": {},
   "source": [
    "QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "Q,R,P = la.qr(X_train, pivoting=True, mode='economic') #economic -> Q is m x k, R is k x n where k=min(m,n)\n",
    "print(\"Condition number of QR factorization matrix: \",np.linalg.cond(R,2))\n",
    "#truncate R and Q to rank\n",
    "R_trunc = R[:rank, :rank]\n",
    "Q_trunc = Q[:, :rank]\n",
    "QTb = Q_trunc.T @ y_train\n",
    "theta_permuted=np.zeros(X_train.shape[1])\n",
    "theta_permuted [:rank]= la.solve_triangular(R_trunc, QTb) #store the solution in the first rank columns\n",
    "#restore original order\n",
    "theta = np.zeros_like(theta_permuted)\n",
    "theta[P] = theta_permuted #permute the elements of theta_permuted to get the solution\n",
    "\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#THIS model is quite good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf7ead",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "print(\"Rank of X_train: \",rank)\n",
    "print(\"Number of non zero singular values: \",np.count_nonzero(S))\n",
    "#S contains all singular values. If we consider only the non zero singular values, they should be equal to the rank of X_train. In this case this is not true, probably due to numerical errors.\n",
    "# I will use later the PCR and Euckardt-Young theorem to remove noise\n",
    "\n",
    "\n",
    "# So let's consider the first r=rank(X_train) singular values\n",
    "U_r = U[:, :rank]\n",
    "Vt_r = Vt[:rank, :]\n",
    "S_r = np.diag(S)[:rank, :rank]\n",
    "S_r_inv = np.linalg.inv(S_r)\n",
    "theta = Vt_r.T @ S_r_inv @ U_r.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8f6f0",
   "metadata": {},
   "source": [
    "LTSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "conditions=[1e-1,1e-2,1e-4,1e-8,1e-12,1e-16]\n",
    "values={}\n",
    "for condition in conditions:\n",
    "    #gelsd\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsd = la.lstsq(X_train, y_train,lapack_driver='gelsd',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    \n",
    "    values['driver']='gelsd'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsd\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsd\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelss\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelss = la.lstsq(X_train, y_train,lapack_driver='gelss',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelss'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelss\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelss\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)\n",
    "    #gelsy\n",
    "    times=[]\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        theta_gelsy = la.lstsq(X_train, y_train,lapack_driver='gelsy',cond=condition)[0] #we took [0] because lstsq returns a tuple\n",
    "        times.append(time.time()-start_time)\n",
    "    values['driver']='gelsy'\n",
    "    values['condition']=condition\n",
    "    values['AVGtime']=np.mean(times)\n",
    "    y_train_pred = X_train @ theta_gelsy\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    values['residuals_train']=np.linalg.norm(residuals_train,2)\n",
    "    values['Min error residuals_train']=np.min(np.abs(residuals_train))\n",
    "    values['Max error residuals_train']=np.max(np.abs(residuals_train))\n",
    "    R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "    values['R2_train']=R2_train\n",
    "    y_test_pred = X_test @ theta_gelsy\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    values['residuals_test']=np.linalg.norm(residuals_test,2)\n",
    "    values['Min error residuals_test']=np.min(np.abs(residuals_test))\n",
    "    values['Max error residuals_test']=np.max(np.abs(residuals_test))\n",
    "    R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "    values['R2_test']=R2_test\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab430e2",
   "metadata": {},
   "source": [
    "PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: mixed error criterion\n",
    "tol=1e-6\n",
    "k=np.argmax(S/(S[0]+1)<tol)-1  #argmax returns the first occurrence of the that satisfies the condition and this is k+1\n",
    "print(\"k=\",k,\" with mixed error criterion and tol=\",tol)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "#2: Scree Plot\n",
    "\n",
    "k1 = KneeLocator(range(len(S)), S, curve='convex', direction='decreasing')\n",
    "plt.plot(range(len(S)), S)\n",
    "plt.scatter(k1.elbow, S[k1.elbow], c='red', s=100, alpha=0.5)\n",
    "plt.xlabel('Singular value index')\n",
    "plt.ylabel('Singular value')\n",
    "plt.title('Scree plot')\n",
    "plt.legend(['Singular values', 'Elbow'])\n",
    "plt.show()\n",
    "k=k1.elbow\n",
    "print(\"k=\",k,\" with Screep plot\")\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n",
    "\n",
    "#3: Cumulative percentage of variance\n",
    "p=0.99\n",
    "total_sum=np.sum(S**2)\n",
    "cumulative_sum=np.cumsum(S**2)\n",
    "k=np.argmax((cumulative_sum/total_sum)>p)\n",
    "print(\"k=\",k,\"with cumulative percentage of variance and p=\",p)\n",
    "print(\"Sigma_k=\",S[k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "S_k = np.diag(S)[:k, :k]\n",
    "S_k_inv = np.linalg.inv(S_k)\n",
    "theta = Vt_k.T @ S_k_inv @ U_k.T @ y_train\n",
    "y_train_pred = X_train @ theta\n",
    "residuals_train = y_train - y_train_pred\n",
    "print(\"Residuals for train set: \",np.linalg.norm(residuals_train,2))\n",
    "print(\"Maximum error for train set: \",np.max(np.abs(residuals_train)))\n",
    "print(\"Minimum error for train set: \",np.min(np.abs(residuals_train)))\n",
    "R2_train  = 1 - np.sum((y_train - y_train_pred)**2)/np.sum((y_train - np.mean(y_train))**2)\n",
    "print(\"R2 for train set: \",R2_train)\n",
    "\n",
    "y_test_pred = X_test @ theta\n",
    "residuals_test = y_test - y_test_pred\n",
    "print(\"Residuals for test set: \",np.linalg.norm(residuals_test,2))\n",
    "print(\"Maximum error for test set: \",np.max(np.abs(residuals_test)))\n",
    "print(\"Minimum error for test set: \",np.min(np.abs(residuals_test)))\n",
    "R2_test  = 1 - np.sum((y_test - y_test_pred)**2)/np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R2 for test set: \",R2_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
