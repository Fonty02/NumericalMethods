{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbbec92-0280-4823-9bb0-40cdb27db96e",
   "metadata": {},
   "source": [
    "\n",
    "# Regression model\n",
    "# Relative location of CT slices on axial axis\n",
    "\n",
    " The data are available at:\n",
    "\n",
    " https://archive.ics.uci.edu/dataset/206/relative+location+of+ct+slices+on+axial+axis\n",
    "\n",
    "The dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body.\n",
    "\n",
    "The data was retrieved from a set of 53500 CT images from 74 different patients (43 male, 31 female).\n",
    "\n",
    "To exstract the data use the panda routines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d786fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patientId  value0  value1  value2  value3  value4  value5  value6  value7  \\\n",
      "0          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "1          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "2          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "3          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "4          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "\n",
      "   value8  ...  value375  value376  value377  value378  value379  value380  \\\n",
      "0   -0.25  ...     -0.25  0.980381       0.0       0.0       0.0       0.0   \n",
      "1   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "2   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "3   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "4   -0.25  ...     -0.25  0.976833       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   value381  value382  value383  reference  \n",
      "0       0.0     -0.25     -0.25  21.803851  \n",
      "1       0.0     -0.25     -0.25  21.745726  \n",
      "2       0.0     -0.25     -0.25  21.687600  \n",
      "3       0.0     -0.25     -0.25  21.629474  \n",
      "4       0.0     -0.25     -0.25  21.571348  \n",
      "\n",
      "[5 rows x 386 columns]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    " \n",
    "# read the dataset using the compression zip\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/static/public/206/relative+location+of+ct+slices+on+axial+axis.zip',compression='zip')\n",
    " \n",
    "# display dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e76930",
   "metadata": {},
   "source": [
    "We transform the data to a matrix of shape 53500 x 386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1be17b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53500, 386)\n"
     ]
    }
   ],
   "source": [
    "Aall=df.to_numpy()\n",
    "print(Aall.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c2aa7",
   "metadata": {},
   "source": [
    "We add a column of all 1 and we organize the input data by dividing in test set and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed970f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#Add a column of ones at the beginning of the data matrix\n",
    "Aall = np.column_stack([np.ones(Aall.shape[0]), Aall])\n",
    "X = Aall\n",
    "X=np.delete(X,386,1)\n",
    "y = Aall[:,386]\n",
    "print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2188d64d-d9f0-4541-b476-08407c30f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = .9,\n",
    "    test_size = .1,\n",
    "    random_state = 5,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b40466b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48150, 386)\n",
      "(5350, 386)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ddcc6-4e55-439f-9e8f-d170cb5c3568",
   "metadata": {},
   "source": [
    "Use the prepared data to solve the regression model with all the studied techniques.\n",
    "Can we use the normal equation and the QR factorization? If the answer is positive compare the condition numbers of the QR methods and the normal equations. What are the results?\n",
    "\n",
    "\n",
    "Use the funcation scipy.linalg.lstsq and check if all the lapack drivers works.\n",
    "Compare the results changing the initial value cond. The results are the same? What about the execution time?\n",
    "\n",
    "Analyze the singular values and check if it is possible to use a principal component regression procedure. Compute the solution using the singular value decomposition. \n",
    "Can you observe a relation in the chosen singular value and the value of cond of the routine lstsq?\n",
    "\n",
    "Perform the same analysis by preprocessing the data in order to have data from a normal distribution with mean zero  and compute the singular value decomposition on this matrix.\n",
    "\n",
    "Check the performance of the method by computing the least square residual for the training set and the testset. The minimum and the maximum values of the predicted error for both, the training set and the testset.\n",
    "\n",
    "Compute the multiple R-squared: R2_train = 1 - sum( (y - yest)**2)/sum( (y-mean(y))**2 where y are the value to predict and yest are the estimated values for the training set.\n",
    "Compute the value R2_test for the testset.\n",
    "\n",
    "A value of R2 near one means that the constructed model is good.\n",
    "\n",
    "Change the size of the training set and the testing set to 0.7% and 0.3% and repeat the previous steps.\n",
    "\n",
    "Comment the obtained results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e128bbb",
   "metadata": {},
   "source": [
    "<h1> Normal Equation </h1>\n",
    "Normal equation is a method to solve the linear regression problem. It is based on the following formula: \n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "where $\\theta$ is the vector of parameters, $X$ is the matrix of input data, and $y$ is the vector of output data. It is important to note that the matrix $X$ must be full rank, otherwise the inverse of $X^T X$ does not exist, so the normal equation cannot be used.\n",
    "\n",
    "\n",
    "Since the matrix $X$ is not full rank, we cannot use the normal equation to solve the problem. The rank of the matrix $X^T X$ is 375, which is lower than p=min(n,m)=386. The condition number of the matrix $X^T X$ is very high, which means that the matrix is ill-conditioned. The condition number of a matrix is computer as follows: \n",
    "\n",
    "$$\n",
    "\\|A\\|_2 \\cdot \\|A^{-1}\\|_2\n",
    "$$\n",
    "We can notice that, if we try, the residual error is very high \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7798edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of train data:  375\n",
      "Shape of train data:  (386, 386)\n",
      "Residuals:  5.583855310705728e+19\n",
      "Condition number of normal equation matrix:  1.8704153273192688e+22\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank of train data: \",np.linalg.matrix_rank(X_train.T @ X_train))\n",
    "print(\"Shape of train data: \",(X_train.T @ X_train).shape)\n",
    "theta = np.linalg.solve(X_train.T @ X_train, X_train.T @ y_train)\n",
    "print(\"Residuals: \",np.linalg.norm(X_train @ theta - y_train))\n",
    "print(\"Condition number of normal equation matrix: \",np.linalg.cond(X_train.T @ X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6bc6",
   "metadata": {},
   "source": [
    "<QR factorization>\n",
    "<!-- Write a description of the problem here, with used formulas IN MARKDOWN -->\n",
    "The QR factorization is a method to solve the linear regression problem. It is based on the following formula:\n",
    "\n",
    "$$\n",
    "X = QR\n",
    "$$\n",
    "\n",
    "where $X$ is the matrix of input data, $Q$ is an orthogonal matrix, and $R$ is an upper triangular matrix. The solution of the linear regression problem is given by:\n",
    "\n",
    "$$\n",
    "\\theta = R^{-1} Q^T y\n",
    "$$\n",
    "\n",
    "Since $X$ is not full rank, we cannot use the QR factorization to solve the problem. We need to use the Pivot QR factorization, which is based on the following formula:\n",
    "\n",
    "$$\n",
    "X P = QR\n",
    "$$\n",
    "\n",
    "where $P$ is a permutation matrix. The solution of the linear regression problem is given by:\n",
    "\n",
    "$$\n",
    "\\theta = R^{-1} Q^T P^T y\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
